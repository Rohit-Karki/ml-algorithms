{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<body>\n",
    "<h2>Project 5: Bias Variance Trade-Off</h2>\n",
    "\n",
    "<!--announcements-->\n",
    "<blockquote>\n",
    "    <center>\n",
    "    <a href=\"http://blogs.worldbank.org/publicsphere/files/publicsphere/biased_processing.jpg\"><img src=\"https://raw.githubusercontent.com/aevanchen/machine_learning_miniprojects/5363ecd7985c9a8de20f9d1b827512280a14ed7e/Bias%20Variance%20Tradeoff/bias.jpg\" width=\"600px\" /></a>\n",
    "    </center>\n",
    "      <p><cite><center>\"All of us show bias when it comes to what information we take in.<br>We typically focus on anything that agrees with the outcome we want.\"<br>\n",
    "<b>--Noreena Hertz</b>\n",
    "      </center></cite></p>\n",
    "</blockquote>\n",
    "<h3>Introduction</h3>\n",
    "\n",
    "<p>\n",
    "Recall that the squared error can be decomposed into <em>bias</em>, <em>variance</em> and <em>noise</em>: \n",
    "</p>\n",
    "$$\n",
    "    \\underbrace{\\mathbb{E}[(h_D(x) - y)^2]}_\\mathrm{Error} = \\underbrace{\\mathbb{E}[(h_D(x)-\\bar{h}(x))^2]}_\\mathrm{Variance} + \\underbrace{\\mathbb{E}[(\\bar{h}(x)-\\bar{y}(x))^2]}_\\mathrm{Bias} + \\underbrace{\\mathbb{E}[(\\bar{y}(x)-y(x))^2]}_\\mathrm{Noise}\\nonumber\n",
    "$$\n",
    "We will now create a data set for which we can approximately compute this decomposition. \n",
    "The function <em><strong>`toydata`</strong></em> generates a binary data set with class $1$ and $2$. Both are sampled from Gaussian distributions:\n",
    "$$\n",
    "p(\\vec x|y=1)\\sim {\\mathcal{N}}(0,{I}) \\textrm { and } p(\\vec x|y=2)\\sim {\\mathcal{N}}(\\mu_2,{I}),\n",
    "$$\n",
    "\n",
    "where $\\mu_2=[2;2]^\\top$ (the global variable <em>OFFSET</em> $\\!=\\!2$ regulates these values: $\\mu_2=[$<em>OFFSET</em> $;$ <em>OFFSET</em>$]^\\top$).\n",
    "\n",
    "<h3>Computing noise, bias and variance</h3>\n",
    "<p>\n",
    "You will need to edit four functions:  <em><strong>`computeybar`</strong></em>,  <em><strong>`computehbar`</strong></em>, and <em><strong>`computevariance`</strong></em>. First take a look at <strong>`biasvariancedemo`</strong> and make sure you understand where each function should be called and how they contribute to the Bias/Variance/Noise decomposition. <br/><br/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l2distance Helper Function: l2distance is a helper function used in our implementation of the ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "import numpy as np\n",
    "from numpy.matlib import repmat\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from scipy.io import loadmat\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`l2distance` Helper Function**: `l2distance` is a helper function used in our implementation of the ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2distance(X, Z=None):\n",
    "    \"\"\"\n",
    "    function D=l2distance(X,Z)\n",
    "\n",
    "    Computes the Euclidean distance matrix.\n",
    "    Syntax:\n",
    "    D=l2distance(X,Z)\n",
    "    Input:\n",
    "    X: dxn data matrix with n vectors (columns) of dimensionality d\n",
    "    Z: dxm data matrix with m vectors (columns) of dimensionality d\n",
    "\n",
    "    Output:\n",
    "    Matrix D of size nxm\n",
    "    D(i,j) is the Euclidean distance of X(:,i) and Z(:,j)\n",
    "\n",
    "    call with only one input:\n",
    "    l2distance(X)=l2distance(X,X)\n",
    "    \"\"\"\n",
    "    if Z is None:\n",
    "        n, d = X.shape\n",
    "        s1 = np.sum(np.power(X, 2), axis=1).reshape(-1,1)\n",
    "        D1 = -2 * np.dot(X, X.T) + repmat(s1, 1, n)\n",
    "        D = D1 + repmat(s1.T, n, 1)\n",
    "        np.fill_diagonal(D, 0)\n",
    "        D = np.sqrt(np.maximum(D, 0))\n",
    "    else:\n",
    "        n, d = X.shape\n",
    "        m, _ = Z.shape\n",
    "        s1 = np.sum(np.power(X, 2), axis=1).reshape(-1,1)\n",
    "        s2 = np.sum(np.power(Z, 2), axis=1).reshape(1,-1)\n",
    "        D1 = -2 * np.dot(X, Z.T) + repmat(s1, 1, m)\n",
    "        D = D1 + repmat(s2, n, 1)\n",
    "        D = np.sqrt(np.maximum(D, 0))\n",
    "    return D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`toydata` Helper Function**: `toydata` is a helper function used to generate the the binary data with n/2 values in class 1 and n/2 values in class 2. With class 1 being the label for data drawn from a normal distribution having mean 0 and sigma 1. And clss 2 being the label for data drawn from a normal distribution with mean OFFSET and sigma 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def toydata(OFFSET,N):\n",
    "    \"\"\"\n",
    "    function [x,y]=toydata(OFFSET,N)\n",
    "    \n",
    "    This function constructs a binary data set. \n",
    "    Each class is distributed by a standard Gaussian distribution.\n",
    "    INPUT: \n",
    "    OFFSET:  Class 1 has mean 0,  Class 2 has mean 0+OFFSET (in each dimension). \n",
    "    N: The function returns N data points ceil(N/2) are of class 2, the rest\n",
    "    of class 1\n",
    "    \"\"\"\n",
    "    \n",
    "    NHALF = int(np.ceil(N/2))\n",
    "    x = np.random.randn(N, 2)\n",
    "    x[NHALF:, :] += OFFSET  \n",
    "    \n",
    "    y = np.ones(N)\n",
    "    y[NHALF:] *= 2\n",
    "    \n",
    "    jj = np.random.permutation(N)\n",
    "    \n",
    "    print (x[jj])\n",
    "    return x[jj, :], y[jj]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "toydata(3,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- <p> -->\n",
    "(a) <strong>Noise:</strong> First we focus on the noise. For this, you need to compute $\\bar y(\\vec x)$ in  <em><strong>`computeybar`</strong></em>. You can compute the probability $p(\\vec x|y)$ with the equations $p(\\vec x|y=1)\\sim {\\mathcal{N}}(0,{I}) \\textrm { and } p(\\vec x|y=2)\\sim {\\mathcal{N}}(\\mu_2,{I})$. Then use Bayes rule to compute $p(y|\\vec x)$. <br/><br/>\n",
    "<strong>Note:</strong> You may want to use the function <em>`normpdf`</em>, which is defined for  you in <em><strong>`computeybar`</strong></em>.\n",
    "<br/><br/>\n",
    "<!-- </p> -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeybar(xTe, OFFSET):\n",
    "    \"\"\"\n",
    "    function [ybar]=computeybar(xTe, OFFSET);\n",
    "\n",
    "    computes the expected label 'ybar' for a set of inputs x\n",
    "    generated from two standard Normal distributions (one offset by OFFSET in\n",
    "    both dimensions.)\n",
    "\n",
    "    INPUT:\n",
    "    xTe : nx2 array of n vectors with 2 dimensions\n",
    "    OFFSET    : The OFFSET passed into the toyData function. The difference in the\n",
    "                mu of labels class1 and class2 for toyData.\n",
    "\n",
    "    OUTPUT:\n",
    "    ybar : a nx1 vector of the expected labels for vectors xTe\n",
    "    \"\"\"\n",
    "    n,temp = xTe.shape\n",
    "    ybar = np.zeros(n)\n",
    "    \n",
    "    # Feel free to use the following function to compute p(x|y), or not\n",
    "    # normal distribution is default mu = 0, sigma = 1.\n",
    "    normpdf = lambda x, mu, sigma: np.exp(-0.5 * np.power((x - mu) / sigma, 2)) / (np.sqrt(2 * np.pi) * sigma)\n",
    "    \n",
    "    ## fill in code here\n",
    "    #raise NotImplementedError('Your code goes here!')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the mean and standard deviation for the Gaussian distribution\n",
    "mean = 0\n",
    "std_dev = 1\n",
    "\n",
    "# Number of data points to generate\n",
    "num_samples = 1000\n",
    "\n",
    "# Generate synthetic data from a Gaussian distribution\n",
    "synthetic_data = np.random.normal(mean, std_dev, num_samples)\n",
    "\n",
    "# Plot a histogram to visualize the distribution\n",
    "print(synthetic_data)\n",
    "plt.hist(synthetic_data, bins=50, density=True, alpha=0.8, color='g')\n",
    "plt.title('Synthetic Data from Gaussian Distribution')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
